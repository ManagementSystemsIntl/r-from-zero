---
title: "Loading and Exploring Data"
subtitle: R from Zero Training Series - Session 1
title-block-banner: "#CACACA"
format: 
  html:
    code-fold: false
    css: styles.css
    toc: true
    embed-resources: true
    smooth-scroll: true
---

```{r, echo=FALSE, include = FALSE}

library(dplyr)
library(purrr)
library(readxl)
library(sjmisc)
library(sjPlot)
library(tidyr)
library(knitr)
library(rmarkdown)
library(readr)
```

# Introduction

This series is designed to help participants build a solid foundation in R programming, regardless of their current skill level. Throughout the training series, we will connect each session to steps in the data analysis workflow.

Note, participants should have already created a GitHub account and installed RStudio on their machine or set up with Posit Cloud. If you missed our previous session on getting set up in R, please review the step-by-step guides and recording [saved here](https://msiworldwide.egnyte.com/fl/wJHfQhg9Q7).

Completing those sessions is required to continue successfully in the series. However, getting the repository cloned to your workspace or machine is not required! Feel free to create your own project for this training and save all the files we share there. Please reach out if you need any support!

If you are working in the repository, please do a pull to update your workspace with items that have been pushed to the repository. Under the participant files, please go ahead and create a new folder for your scripts.

# Session 1: Loading and Exploring Data

Now, to kick off this series, we will cover loading and exploring a dataset in R. In this session, we will focus on the initial steps necessary to ensure you are working with a clean dataset.

In this session, we will cover:

-   Ideal Folder Structure

-   Installing and loading packages

-   Setting the working directory

-   Loading data and creating objects

-   Basic functions to explore data

# Ideal Folder Structure

When preparing for an analysis it is essentially to develop a proper folder structure as well as a proper workflow for your analysis. A well-organized folder structure helps maintain clarity throughout your analysis process.

> Reproducibility should be the cornerstone of any data analysis workflow. This means that anyone should be able to replicate your results using the same data and methods.

The common folders used for an analysis task are:

1.  A "data" folder to store data files, with subfolders for each type of data, such as raw, cleaned and prepared.

2.  A "scripts" folder to store R scripts.

3.  An "output" folder to store plots, figures and tables, with subfolders for each type of relevant output (e.g., "viz","tables", "models").

4.  A "background docs" folder to store any background documentation or literature reviews.

5.  An "instruments" folder to store any instruments or questionnaires used.

6.  A "reports" folder to store any reports or presentations related the task.

::::: box
::: box-header
Data Analysis Workflow
:::

::: box-container
The data science team has developed a workflow that has proven effective for our data analysis activities. We utilize R scripts, R Markdowns or Quarto Documents to create files for each step in the workflow. The common steps the data science team uses are:

00 prep - sets the stage for the analysis, installs packages, loads libraries, sets themes and colors, creates labels, creates the dataset objects and more.

01 clean - reviews the dataset to check for missing values, errors, anomalies in the data, as well as making small adjustments to address any identified issues.

02 prepared - involves transformation techniques to create new variables that are needed for analysis.

03 explore - uses exploratory data analysis (EDA) techniques to understand the data's structure, patterns, and relationships.

04 analysis - applies statistical methods and models to draw insights from the data.

This is the core structure of a data analysis workflow but can be adapted to fit the needs of your analysis activity.
:::
:::::

# Installing and Loading Packages

When you start an analysis activity, the first script you will create would be your prep script (as mentioned above). This script will eventually be used for a `source()` function in other scripts in the workflow process to read and execute R code from that file. We will get to all this later.

> To create a new script, go to the green plus sign on the upper left of the screen and select R Script. Go ahead and save this file as "00 prep" in the folder you created for yourself under participant files

One of the key things you want to include in the prep script are packages needed for your analysis. The first time you use a package in R you have to install it. This is done by using the `install.packages()` function, where you specify the name of the package as a string. For example, to install the dplyr package, you would use:

`install.packages("dplyr")`

After installation, to make the functions and datasets of a package available in your R session, you need to load the package using the `library()` function: `library(dplyr)`

There are more than one package needed for this session, and there generally is for any analytical activity. Thankfully, you don't have to install each package one at a time. You can use the `c()` function to combine several values or objects into a vector which allows us to use the function only one time. Please add these lines of code to your 00 prep script. Remember to add notes as you go (`#` comment) so you can notate your workflow.

**Install required packages:**

`install.packages(c(``"dplyr",``"purrr", "readxl", "sjmisc", "sjPlot"))`

**Load libraries:**

`library(dplyr)`

`library(purrr)`

`library(readxl)`

`library(sjmisc)`

`library(sjPlot)`

# Loading the Data

Now that the packages are loaded, we can read the data and add it into the environment. When the data is stored in our local files, we can call the file using the appropriate file path. First, it is important to set your home working directory, and then call the file name.

> R is capable of reading data in many formats, but the user has to know what the format of the file is and where the file is stored.

Set a working directory by using the `setwd()` function or go to Session/Set Working Directory/To Project Directory. If you are working in Posit Cloud, the default is `setwd("/cloud/project")`

An example of a working directory is, `setwd("C:/Users/melanie.murphy/Desktop/R projects/r-from-zero")`

Now, we want to assign the dataset to the object named "raw" since this is the raw dataset. This creates an object called "raw" in the top right of your screen in the Environment.

```{r}
raw <- read.csv("week 1/data/dataset.csv")
```

> In general, we usually create the object of the current dataset to the 00 prep file. As you get further into your analysis process, the name of the object and the file you use will change. More on that later.

# Explore the Data

But not the kind of exploring I previously mentioned where we are conducting EDA. Here we will look at the raw dataset in a few different ways to get an idea of what we are working with.

> Before we get into that, create a another script and save this file as "01 clean" in the folder you created for yourself under participant files. We will finish the session in this file.

First, let's look at some of the properties of our data using `glimpse()`

```{r}
glimpse(raw)
```

Now letâ€™s take a look at the first few rows of our data using `head()`

```{r}
head(raw)
```

Then the last rows using `tail()`

```{r}
tail(raw)
```

Finally, we can look at the variable names in the dataset using `names()`

```{r}
names(raw)
```

# Frequency Review

Before the analysis begins, we need to verify that the data are accurate and that the variables are properly labelled, have the correct values, have proper data types, etc. This is what we will focus on for the rest of this session.

> It is important to work in opportunities to conduct this review before receiving the final dataset. Whether from a household survey or third-party monitoring data, it is very important to conduct a pilot test or test the tool yourself to ensure you capture errors early on. By the time you receive the final dataset, there shouldn't be any substantial errors like the ones we will cover. But these are the types of errors you may find during the pilot or testing phase.

There are a few key functions you could use when conducting a review on the raw dataset.

`filter()`: This function is used to subset rows in a dataset based on specific conditions. For example, you might want to filter out responses from participants who did not consent to the survey.

```{r}
filtered_data <- raw %>% 
  filter(consent == "Yes")

head(filtered_data)
```

You can also filter with numeric values such as age.

```{r}
under_retireage <- raw %>% 
  filter(age<62)

head(under_retireage)
```

`select()`: This function allows you to subset columns in a dataset. This is useful when you want to focus on specific variables for your analysis.

```{r}
questionsA <- raw %>% 
  select(ID:employment)

head(questionsA)
```

You can also select the variables this way!

```{r}
questionsB <- raw %>% 
  select(consent, registered, received_assistance, satisfaction, dissatisfied_reason, quantity,      quality)

head(questionsB)
```

`summarize()`: This function manipulates observations based on the value of one or more variables. It is often used to calculate summary statistics such as means, counts, or proportions. Note, `na.rm = TRUE` means you want to exclude the NAs when it calculates the mean.

```{r}
age_mn <- raw %>% 
  summarize(mean_age = mean(age, na.rm = TRUE))

age_mn
```

`frq()`: This function is typically used to generate frequency tables for categorical variables, allowing you to see how many times each category appears in the dataset. In my opinion, this is the easiest function to use when conducting frequency checks.

```{r}
frq(raw$education)
```

`map()`: This function from the purrr package allows you to apply a function to each element of a list or vector. It is useful for applying the same function (like `frq()`) to multiple variables in your dataset. Another one of my favorites.

```{r}
map(questionsB, frq)
```

`tab_xtab()`: This function is used to create cross-tabulations between two categorical variables, allowing you to examine the relationship between them.

```{r, warning=FALSE}
tab_xtab(raw$employment, 
         raw$sex)
```

`table()`: This base R function creates a contingency table of counts for categorical variables. It is a simple way to get another overview of the distribution of a variable.

```{r}
table(raw$received_assistance)
```

# Practice Script

For practice, we are sharing a script that contains a fake survey and corresponding data. Participants should use the common functions above to find as many errors in the dataset as you can!

Through these "frequency checks", you will be able to identify errors that need to be fix in the computer-assisted personal interviewing (CAPI) before data collection starts. Thankfully, this is the pilot data and we still have time to fix them!

**Things to look out for:**

-   Implausible or suspicious values

-   Mistakes in skip logic and question type

-   Failures of logic tests (not skip logic but expected social values such as combinations of age, education, employment, etc)

Please drop any questions in our [Teams Channel](https://teams.microsoft.com/l/channel/19%3Acab5251c965a4b15bc0b8d99fb1ac157%40thread.tacv2/R%20from%20Zero%20Training%20Series%20-%202025?groupId=175a191b-5543-4fc0-81db-926ead8863f1&tenantId=a40fe4ba-abc7-48fe-8792-b43889936400) if you need support! Good luck!
